{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Name: The Big One\n",
    "\n",
    "## Group Members: Nicholas Parker, Matthew King, Sean Sturtevant\n",
    "\n",
    "### Dataset: Predict NHL Player Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Ask\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we accurately predict the salary of a player in the NHL, for the 2016-2017 season?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Acquire\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to data and data dictionary can be found [here](https://www.kaggle.com/camnugent/predict-nhl-player-salaries#train.csv).\n",
    "\n",
    "The data contains 874 records of NHL players and 151 features included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Process\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Direct Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hockey_train = pd.read_csv('./data/clean/train.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "hockey_test = pd.read_csv('./data/clean/test.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "hockey_test_y = pd.read_csv('./data/clean/test_salaries.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Kaggle dataset gives the data split in train and test\n",
    "# We will recombine and make our own train and test split\n",
    "def combine_train_and_test(train_df, test_df, test_response):\n",
    "    \"\"\"\n",
    "    Combine the train and test datasets that were previous split at the source of the data.\n",
    "    \"\"\"\n",
    "    test_df = pd.concat([test_df, test_response], axis = 1)\n",
    "    return pd.concat([train_df, test_df],ignore_index = True, sort = False)\n",
    "\n",
    "hockey = combine_train_and_test(hockey_train, hockey_test, hockey_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the time variable 'Born' by making a variable 'Age'\n",
    "hockey['Age'] = 117 - pd.to_numeric(hockey['Born'].str[0:2])\n",
    "hockey.loc[(hockey['Age'] > 99), 'Age'] = hockey['Age'] - 100\n",
    "\n",
    "def nationality_group(df, nationalityCol):\n",
    "    \"\"\"\n",
    "    Reduces the number of values in the Nationality column.\n",
    "    \"\"\"\n",
    "    # A function to feature engineering the 'Nationality column'\n",
    "    # Changes it from 16 unique values to 5 to prevent overfitting\n",
    "    scandanavianNations = ['SWE','NOR','FIN']\n",
    "    otherNations = ['CHE','CZE','FRA','DEU','SVK','AUT','DNK','LVA','HRV','GBR','SVN']\n",
    "    df.loc[(df[nationalityCol].isin(scandanavianNations)), nationalityCol] = 'Scandanavian'\n",
    "    df.loc[(df[nationalityCol].isin(otherNations)), nationalityCol] = 'Other'\n",
    "    return df\n",
    "\n",
    "hockey = nationality_group(hockey, 'Nat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to group and remove provinces and states that are only seen a few times\n",
    "# Useful to prevent overfitting\n",
    "prs = hockey.groupby('Pr/St').agg({'Pr/St':['count']}).reset_index()\n",
    "prs.columns = ['pr/st','count']\n",
    "extreneousStates = list(prs.loc[(prs['count'] < 10)]['pr/st'])\n",
    "hockey.loc[(hockey['Pr/St'].isin(extreneousStates)),'Pr/St'] = 'Other'\n",
    "hockey.loc[(hockey['Pr/St'].isna()),'Pr/St'] = 'UnSpecified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding isNa Cols\n",
    "# These columns are useful to account for missing data\n",
    "def addIsNACol(df, col_name):\n",
    "    \"\"\"\n",
    "    Add columns that indicate whether a record had missing data for specified features.\n",
    "    \"\"\"\n",
    "    na_col_name = col_name + '_is_na'\n",
    "    df[na_col_name] = 0\n",
    "    df.loc[(df[col_name].isna()), na_col_name] = 1\n",
    "    return df\n",
    "\n",
    "hockey = addIsNACol(hockey, 'DftYr')\n",
    "hockey = addIsNACol(hockey, 'iCF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Processed Data to be used by Model Pipeline\n",
    "Further work with the columns will be done in the pipeline by excluding variable and imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hockey.to_csv('./data/processed/hockey.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Exploratory Data Analysis\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Models\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import compose\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn import impute\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_commas(number):\n",
    "    \"\"\"\n",
    "    Adds commas to values greater that 1,000 for evaluation metrics.\n",
    "    \"\"\"\n",
    "    return (\"{:,}\".format(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_metric(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean absolute percentage error.\n",
    "    \"\"\"\n",
    "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "    n = len(y_test)\n",
    "    running_sum = 0\n",
    "    for i in range(n):\n",
    "        running_sum += abs((y_test[i] - y_pred[i])/y_test[i])\n",
    "    return running_sum/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hockey['Salary']\n",
    "X = hockey.drop('Salary', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_baseline = [feature for feature in X_train.columns if np.issubdtype(X_train[feature], np.number)]\n",
    "categorical_baseline = [feature for feature in X_train.columns if feature not in numeric_baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('numerical',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='median',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler(copy=True,\n",
       "                                                                                  wit...\n",
       "                                                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                                                 handle_unknown='ignore',\n",
       "                                                                                 n_values=None,\n",
       "                                                                                 sparse=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['Born', 'City', 'Pr/St',\n",
       "                                                   'Cntry', 'Nat', 'Hand',\n",
       "                                                   'Last Name', 'First Name',\n",
       "                                                   'Position', 'Team'])],\n",
       "                                   verbose=False)),\n",
       "                ('regressor',\n",
       "                 Ridge(alpha=100, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='auto', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_pipeline_ridge(regressor=None):\n",
    "    \"\"\"\n",
    "    Creates pipeline to perform transformations on numeric and categorical features and pass into a \n",
    "    Ridge Regression Model.\n",
    "    \"\"\"\n",
    "    numeric_features = numeric_baseline\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', impute.SimpleImputer(strategy='median')),\n",
    "        ('scaler', preprocessing.StandardScaler())])\n",
    "\n",
    "    categorical_features = categorical_baseline\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', impute.SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "        ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = compose.ColumnTransformer(transformers=[\n",
    "        ('numerical', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', regressor)])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "regressor = linear_model.Ridge(alpha=100, tol=0.001)\n",
    "pipeline = make_pipeline_ridge(regressor)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$612,384.78 medae on train dataset\n",
      "$637,503.8 medae on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_train)\n",
    "medae_value_train = add_commas(round(metrics.median_absolute_error(y_train, y_pred), 2))\n",
    "print(f\"${medae_value_train} medae on train dataset\")\n",
    "\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "medae_value_validation = add_commas(round(metrics.median_absolute_error(y_validation, y_pred), 2))\n",
    "print(f\"${medae_value_validation} medae on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1,233,196.65 rmse on train dataset\n",
      "$1,297,590.24 rmse on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_train)\n",
    "rmse_value_train = add_commas(round(np.sqrt(metrics.mean_squared_error(y_train, y_pred)), 2))\n",
    "print(f\"${rmse_value_train} rmse on train dataset\")\n",
    "\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "rmse_value_validation = add_commas(round(np.sqrt(metrics.mean_squared_error(y_validation, y_pred)), 2))\n",
    "print(f\"${rmse_value_validation} rmse on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.24% mape on train dataset\n",
      "69.28% mape on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_train)\n",
    "mape_value_train = round(mape_metric(y_train, y_pred)*100, 2)\n",
    "print(f\"{mape_value_train}% mape on train dataset\")\n",
    "\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "mape_value_validation = round(mape_metric(y_validation, y_pred)*100, 2)\n",
    "print(f\"{mape_value_validation}% mape on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpredictive_features = ['ENG', 'Wide', 'Over', 'PSG', 'PSA', 'S.Dflct', 'G.Bkhd', 'Post', 'G.Dflct', 'CBar ', 'G.Slap', 'G.Snap', 'G.Wrst', 'G.Wrap', 'G.Tip', 'S.Bkhd', 'Min', 'S.Slap', 'Misc', 'Noise', 'DAP', 'Grit', 'PS', 'DPS', 'OPS', 'DSA', 'DSF', 'Game', 'Match', 'S.Snap', 'Maj', '1G', 'NPD', 'iPenDf', 'iPenD', 'iPenT', 'S.Wrst', 'S.Wrap', 'S.Tip', 'GWG', 'FOL.Down', 'OTG', 'PIM', 'iSF.1', 'iCF.1', 'Diff', 'Pct%', 'FOL.Close', 'TOI/GP.1', 'TOI/GP', 'TOI', 'Shifts', 'E+/-', 'sDist', '+/-', 'PTS', 'A2', 'A1', 'A', 'G', 'GP', 'Wt', 'Ht', 'iSF.2', 'Age', 'iFOW', 'iBLK', 'iFOL', 'dzFOL', 'nzFOW', 'nzFOL', 'ozFOW', 'ozFOL', 'dzFOW', 'FOL.Up', 'FOW.Up', 'iTKA', 'iGVA', 'iMiss', 'FOW.Down', 'iHF', 'FOW.Close', 'FO%', 'Position', 'Team', 'PEND', 'TOIX', 'GA', 'xGA', 'iSCF', 'iPEND', 'sDist.1', 'iHF.1', 'PDO', 'Hand', 'SCA', 'iTKA.1', 'SA', 'IPP%', 'ixG', 'FA', 'Pace', 'iGVA.1', 'SV%', 'RBF', 'PENT', 'F/60', 'GVA', 'TKA', 'FOW', 'Diff/60']\n",
    "\n",
    "# For KNN, we only want to include the numeric variables.\n",
    "# At the moment we do not want to consider some of the categorical \n",
    "# variables since KNN will not handle these naturally.\n",
    "numeric_knn = [feature for feature in X_train.columns if np.issubdtype(X_train[feature], np.number) \n",
    "                      and feature not in nonpredictive_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to fit the knn model, we will need to make sure that our pipeline is suited to handle the data for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline_knn(knn=None):\n",
    "    \"\"\"\n",
    "    Creates pipeline that performs separate transformations on the categorical and numerical features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # All predictive numeric features\n",
    "    numeric_features = numeric_knn\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        # Impute any missing values with the median.\n",
    "        ('imputer', impute.SimpleImputer(strategy='median')),\n",
    "        # Make sure that all values are Normalized for KNN. \n",
    "        # Since we are going to look at points that are similar in\n",
    "        # terms of euclidean space, we need all features on the same\n",
    "        # scale.\n",
    "        ('normalizer', preprocessing.Normalizer())])\n",
    "\n",
    "    preprocessor = compose.ColumnTransformer(transformers=[\n",
    "        ('numerical', numeric_transformer, numeric_features)])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('knn', knn)])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "pipeline_knn = make_pipeline_knn(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    0.9s finished\n",
      "/Users/MatthewKing/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('preprocessor',\n",
       "                                              ColumnTransformer(n_jobs=None,\n",
       "                                                                remainder='drop',\n",
       "                                                                sparse_threshold=0.3,\n",
       "                                                                transformer_weights=None,\n",
       "                                                                transformers=[('numerical',\n",
       "                                                                               Pipeline(memory=None,\n",
       "                                                                                        steps=[('imputer',\n",
       "                                                                                                SimpleImputer(add_indicator=False,\n",
       "                                                                                                              copy=True,\n",
       "                                                                                                              fill_value=None,\n",
       "                                                                                                              missing_values=nan,\n",
       "                                                                                                              st...\n",
       "                                                                  metric_params=None,\n",
       "                                                                  n_jobs=None,\n",
       "                                                                  n_neighbors=5,\n",
       "                                                                  p=2,\n",
       "                                                                  weights='uniform'))],\n",
       "                                      verbose=False),\n",
       "                   iid='warn', n_iter=15, n_jobs=None,\n",
       "                   param_distributions={'knn__algorithm': ['ball_tree',\n",
       "                                                           'kd_tree', 'auto'],\n",
       "                                        'knn__n_neighbors': [8, 9, 10, 15],\n",
       "                                        'knn__weights': ['distance',\n",
       "                                                         'uniform']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_random_cv_knn():\n",
    "    \"\"\"\n",
    "    Define hyperparameter search space\n",
    "    Instantiate RandomizedSearchCV with the pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    algo = ['ball_tree', 'kd_tree', 'auto']\n",
    "    weights = ['distance', 'uniform']\n",
    "    neighbors = [8, 9, 10, 15]\n",
    "    hyperparameters = dict(knn__algorithm=algo,\n",
    "                          knn__n_neighbors=neighbors,\n",
    "                          knn__weights=weights)\n",
    "    \n",
    "    reg_random_cv = RandomizedSearchCV(pipeline_knn, \n",
    "                                       hyperparameters, \n",
    "                                       cv=5, \n",
    "                                       n_iter=15, \n",
    "                                       verbose=1,\n",
    "                                       random_state=42)\n",
    "    \n",
    "    return reg_random_cv\n",
    "\n",
    "model_knn = make_random_cv_knn()\n",
    "model_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 677416.66666667,  647333.33333333,  716666.66666667,\n",
       "        449833.33333333,  773333.33333333,  588833.33333333,\n",
       "       1091666.66666667,  521833.33333333,  420000.        ,\n",
       "        724333.33333333])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model.best_estimator_, \n",
    "                X_train, \n",
    "                y_train, \n",
    "                scoring=median_absolute_error_scorer,\n",
    "                cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$567,833.33 medae on train dataset\n",
      "$478,780.0 medae on test dataset\n"
     ]
    }
   ],
   "source": [
    "knn_y_pred = model.best_estimator_.predict(X_train)\n",
    "medae_value_train = add_commas(round(metrics.median_absolute_error(y_train, knn_y_pred), 2))\n",
    "print(f\"${medae_value_train} medae on train dataset\")\n",
    "\n",
    "knn_y_pred = model.best_estimator_.predict(X_test)\n",
    "medae_value_test = add_commas(round(metrics.median_absolute_error(y_test, knn_y_pred), 2))\n",
    "print(f\"${medae_value_test:} medae on test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1,635,517.17 rmse on train dataset\n",
      "$1,629,444.5 rmse on test dataset\n"
     ]
    }
   ],
   "source": [
    "knn_y_pred = model.best_estimator_.predict(X_train)\n",
    "rmse_value_train = add_commas(round(np.sqrt(metrics.mean_squared_error(y_train, knn_y_pred)), 2))\n",
    "print(f\"${rmse_value_train} rmse on train dataset\")\n",
    "\n",
    "knn_y_pred = model.best_estimator_.predict(X_test)\n",
    "rmse_value_test = add_commas(round(np.sqrt(metrics.mean_squared_error(y_test, knn_y_pred)), 2))\n",
    "print(f\"${rmse_value_test} rmse on test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.52% mape on train dataset\n",
      "56.01% mape on test dataset\n"
     ]
    }
   ],
   "source": [
    "knn_y_pred = model.best_estimator_.predict(X_train)\n",
    "mape_value_train = round(mape_metric(y_train, knn_y_pred)*100, 2)\n",
    "print(f\"{mape_value_train}% mape on train dataset\")\n",
    "\n",
    "knn_y_pred = model.best_estimator_.predict(X_test)\n",
    "mape_value_test = round(mape_metric(y_test, knn_y_pred)*100, 2)\n",
    "print(f\"{mape_value_test}% mape on test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpredictive_features = ['Born', 'City', 'Last Name', 'First Name', 'Cntry']\n",
    "# negImportanceDropList = ['Position', 'Team', 'PEND', 'TOIX', 'GA', 'xGA', 'iSCF', 'iPEND', 'sDist.1', 'iHF.1', 'PDO', 'Hand', 'SCA', 'iTKA.1', 'SA', 'IPP%', 'ixG', 'FA', 'Pace', 'iGVA.1', 'SV%', 'RBF', 'PENT', 'F/60', 'GVA', 'TKA', 'FOW', 'Diff/60']\n",
    "# nonpredictive_features = ['ENG', 'Wide', 'Over', 'PSG', 'PSA', 'S.Dflct', 'G.Bkhd', 'Post', 'G.Dflct', 'CBar ', 'G.Slap', 'G.Snap', 'G.Wrst', 'G.Wrap', 'G.Tip', 'S.Bkhd', 'Min', 'S.Slap', 'Misc', 'Noise', 'DAP', 'Grit', 'PS', 'DPS', 'OPS', 'DSA', 'DSF', 'Game', 'Match', 'S.Snap', 'Maj', '1G', 'NPD', 'iPenDf', 'iPenD', 'iPenT', 'S.Wrst', 'S.Wrap', 'S.Tip', 'GWG', 'FOL.Down', 'OTG', 'PIM', 'iSF.1', 'iCF.1', 'Diff', 'Pct%', 'FOL.Close', 'TOI/GP.1', 'TOI/GP', 'TOI', 'Shifts', 'E+/-', 'sDist', '+/-', 'PTS', 'A2', 'A1', 'A', 'G', 'GP', 'Wt', 'Ht', 'iSF.2', 'Age', 'iFOW', 'iBLK', 'iFOL', 'dzFOL', 'nzFOW', 'nzFOL', 'ozFOW', 'ozFOL', 'dzFOW', 'FOL.Up', 'FOW.Up', 'iTKA', 'iGVA', 'iMiss', 'FOW.Down', 'iHF', 'FOW.Close', 'FO%', 'Position', 'Team', 'PEND', 'TOIX', 'GA', 'xGA', 'iSCF', 'iPEND', 'sDist.1', 'iHF.1', 'PDO', 'Hand', 'SCA', 'iTKA.1', 'SA', 'IPP%', 'ixG', 'FA', 'Pace', 'iGVA.1', 'SV%', 'RBF', 'PENT', 'F/60', 'GVA', 'TKA', 'FOW', 'Diff/60']\n",
    "# RedundantColDropList = ['TOI/GP', 'iCF', 'iSF', 'iSF.1', 'sDist', 'iHF', 'iGVA', 'iTKA', 'iBLK', 'iFOW', 'iFOL']\n",
    "\n",
    "numeric_rf = [feature for feature in X_train.columns if np.issubdtype(X_train[feature], np.number) \n",
    "                      and feature not in nonpredictive_features]\n",
    "categorical_rf = [feature for feature in X_train.columns if feature not in numeric_rf\n",
    "                       and feature not in nonpredictive_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline_rf(regressor=None):\n",
    "    \"\"\"\n",
    "    Creates pipeline that performs separate transformations on the categorical and numerical features.\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_features = numeric_rf\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', impute.SimpleImputer(strategy='median'))])\n",
    "\n",
    "    categorical_features = categorical_rf\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', impute.SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "        ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = compose.ColumnTransformer(transformers=[\n",
    "        ('numerical', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', regressor)])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "regressor_rf = RandomForestRegressor()\n",
    "pipeline_rf = make_pipeline_rf(regressor_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:  1.5min finished\n",
      "/Users/MatthewKing/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('preprocessor',\n",
       "                                              ColumnTransformer(n_jobs=None,\n",
       "                                                                remainder='drop',\n",
       "                                                                sparse_threshold=0.3,\n",
       "                                                                transformer_weights=None,\n",
       "                                                                transformers=[('numerical',\n",
       "                                                                               Pipeline(memory=None,\n",
       "                                                                                        steps=[('imputer',\n",
       "                                                                                                SimpleImputer(add_indicator=False,\n",
       "                                                                                                              copy=True,\n",
       "                                                                                                              fill_value=None,\n",
       "                                                                                                              missing_values=nan,\n",
       "                                                                                                              st...\n",
       "                   iid='warn', n_iter=15, n_jobs=None,\n",
       "                   param_distributions={'regressor__bootstrap': ['True',\n",
       "                                                                 'False'],\n",
       "                                        'regressor__max_features': ['auto',\n",
       "                                                                    'sqrt'],\n",
       "                                        'regressor__min_samples_leaf': [2, 3, 5,\n",
       "                                                                        6, 7, 8,\n",
       "                                                                        9, 10],\n",
       "                                        'regressor__n_estimators': [100, 150,\n",
       "                                                                    200],\n",
       "                                        'regressor__oob_score': ['True',\n",
       "                                                                 'False']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_random_cv():\n",
    "    \"\"\"\n",
    "    Define hyperparameter search space\n",
    "    Instantiate RandomizedSearchCV with the pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    bootstrap = ['True', 'False']\n",
    "    oob_score = ['True', 'False']\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    min_samples_leaf = [2, 3, 5, 6 , 7, 8, 9, 10]\n",
    "    n_estimators = [100, 150, 200]\n",
    "    hyperparameters = dict(regressor__min_samples_leaf=min_samples_leaf,\n",
    "                          regressor__bootstrap=bootstrap,\n",
    "                          regressor__max_features=max_features,\n",
    "                          regressor__n_estimators=n_estimators,\n",
    "                          regressor__oob_score=oob_score)\n",
    "    reg_random_cv = RandomizedSearchCV(pipeline_rf, \n",
    "                                       hyperparameters, \n",
    "                                       cv=5, \n",
    "                                       n_iter=15, \n",
    "                                       verbose=1,\n",
    "                                       random_state=42)\n",
    "    \n",
    "    return reg_random_cv\n",
    "\n",
    "model_rf = make_random_cv()\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Scores for Best Estimator from Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([462179.49144426, 607508.80362354, 935453.86203336, 561725.64350776,\n",
       "       738106.63887799, 672726.24689349, 742981.16107381, 573301.50436047,\n",
       "       539585.02310011, 369276.45116704])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_absolute_error_scorer = make_scorer(metrics.median_absolute_error)\n",
    "cross_val_score(model_rf.best_estimator_, \n",
    "                X_train, \n",
    "                y_train, \n",
    "                scoring=median_absolute_error_scorer,\n",
    "                cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$330,689.55 medae on train dataset\n",
      "$444,567.18 medae on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_train)\n",
    "medae_value_train = add_commas(round(metrics.median_absolute_error(y_train, y_pred), 2))\n",
    "print(f\"${medae_value_train} medae on train dataset\")\n",
    "\n",
    "y_pred = model_rf.predict(X_validation)\n",
    "medae_value_validation = add_commas(round(metrics.median_absolute_error(y_validation, y_pred), 2))\n",
    "print(f\"${medae_value_validation} medae on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$860,692.11 rmse on train dataset\n",
      "$1,122,580.39 rmse on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_train)\n",
    "rmse_value_train = add_commas(round(np.sqrt(metrics.mean_squared_error(y_train, y_pred)), 2))\n",
    "print(f\"${rmse_value_train} rmse on train dataset\")\n",
    "\n",
    "y_pred = model_rf.predict(X_validation)\n",
    "rmse_value_validation = add_commas(round(np.sqrt(metrics.mean_squared_error(y_validation, y_pred)), 2))\n",
    "print(f\"${rmse_value_validation} rmse on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.88% mape on train dataset\n",
      "49.25% mape on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_train)\n",
    "mape_value_train = round(mape_metric(y_train, y_pred)*100, 2)\n",
    "print(f\"{mape_value_train}% mape on train dataset\")\n",
    "\n",
    "y_pred = model_rf.predict(X_validation)\n",
    "mape_value_validation = round(mape_metric(y_validation, y_pred)*100, 2)\n",
    "print(f\"{mape_value_validation}% mape on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to continue with the Random Forest Model based on the evaluation metrics on the validation sets that we performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Deliver\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Median Absolute Error Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$463,871.38 medae on test dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_test)\n",
    "medae_value_test = add_commas(round(metrics.median_absolute_error(y_test, y_pred), 2))\n",
    "print(f\"${medae_value_test:} medae on test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Squared Error Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1,434,950.04 rmse on test dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_test)\n",
    "rmse_value_test = add_commas(round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)), 2))\n",
    "print(f\"${rmse_value_test} rmse on test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Percentage Error Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.81% mape on test dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_test)\n",
    "mape_value_test = round(mape_metric(y_test, y_pred)*100, 2)\n",
    "print(f\"{mape_value_test}% mape on test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
