{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Name: The Big One\n",
    "\n",
    "## Group Members: Nicholas Parker, Matthew King, Sean Sturtevant\n",
    "\n",
    "### Dataset: Predict NHL Player Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Ask\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we accurately predict the salary of an NHL player, for the 2016-2017 season?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Acquire\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to data and data dictionary can be found [here](https://www.kaggle.com/camnugent/predict-nhl-player-salaries#train.csv).\n",
    "\n",
    "The dataset contains 874 records of NHL players and 151 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Process\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Direct Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hockey_train = pd.read_csv('./data/clean/train.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "hockey_test = pd.read_csv('./data/clean/test.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "hockey_test_y = pd.read_csv('./data/clean/test_salaries.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_train_and_test(train_df, test_df, test_response):\n",
    "    \"\"\"\n",
    "    Combine the train and test datasets that were previous split at the source of the data.\n",
    "    \"\"\"\n",
    "    test_df = pd.concat([test_df, test_response], axis = 1)\n",
    "    return pd.concat([train_df, test_df],ignore_index = True, sort = False)\n",
    "\n",
    "hockey = combine_train_and_test(hockey_train, hockey_test, hockey_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nationality_group(df, nationalityCol):\n",
    "    \"\"\"\n",
    "    Reduces the number of values in the Nationality column through binning.\n",
    "    \"\"\"\n",
    "    # A function to feature engineering the 'Nationality column'\n",
    "    # Changes it from 16 unique values to 5 to prevent overfitting\n",
    "    scandanavianNations = ['SWE','NOR','FIN']\n",
    "    otherNations = ['CHE','CZE','FRA','DEU','SVK','AUT','DNK','LVA','HRV','GBR','SVN']\n",
    "    df.loc[(df[nationalityCol].isin(scandanavianNations)), nationalityCol] = 'Scandanavian'\n",
    "    df.loc[(df[nationalityCol].isin(otherNations)), nationalityCol] = 'Other'\n",
    "    return df\n",
    "\n",
    "hockey = nationality_group(hockey, 'Nat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to group and remove provinces and states that are only seen a few times\n",
    "# Useful to prevent overfitting to values only observed a few times\n",
    "\n",
    "extreneousStates = ['AK', 'AL', 'AZ', 'CO', 'CT', 'FL', 'IN', 'ME', 'MO', 'NC'\n",
    "                    , 'ND', 'NE', 'NH', 'NJ', 'NL', 'NS', 'OH', 'OK', 'PA'\n",
    "                    , 'PE', 'RI', 'SC', 'TX', 'UT', 'WA']\n",
    "\n",
    "hockey.loc[(hockey['Pr/St'].isin(extreneousStates)),'Pr/St'] = 'Other'\n",
    "\n",
    "# Removing the time variable 'Born' by making a variable 'Age'\n",
    "hockey['Age'] = 117 - pd.to_numeric(hockey['Born'].str[0:2])\n",
    "hockey.loc[(hockey['Age'] > 99), 'Age'] = hockey['Age'] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding isNa Cols\n",
    "# These columns are useful to account for missing data\n",
    "def addIsNACol(df, col_name):\n",
    "    \"\"\"\n",
    "    Add columns that indicate whether a record had missing data for specified features.\n",
    "    \"\"\"\n",
    "    na_col_name = col_name + '_is_na'\n",
    "    df[na_col_name] = 0\n",
    "    df.loc[(df[col_name].isna()), na_col_name] = 1\n",
    "    return df\n",
    "\n",
    "hockey = addIsNACol(hockey, 'DftYr')\n",
    "hockey = addIsNACol(hockey, 'iCF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Processed Data to be used by Model Pipeline\n",
    "Further work with the columns will be done in the pipeline by excluding variable and imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hockey.to_csv('./data/processed/hockey.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Exploratory Data Analysis\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Models\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import compose\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn import impute\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_commas(number):\n",
    "    \"\"\"\n",
    "    Adds commas to values greater that 1,000 for evaluation metrics.\n",
    "    \"\"\"\n",
    "    return (\"{:,}\".format(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_metric(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean absolute percentage error.\n",
    "    \"\"\"\n",
    "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "    n = len(y_test)\n",
    "    running_sum = 0\n",
    "    for i in range(n):\n",
    "        running_sum += abs((y_test[i] - y_pred[i])/y_test[i])\n",
    "    return running_sum/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, test, and validation sets of the data.\n",
    "y = hockey['Salary']\n",
    "X = hockey.drop('Salary', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) # Split to obtain the test set\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train) # Split to obtain the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate lists of numeric and categorical features to be passed through the pipeline.\n",
    "numeric_baseline = [feature for feature in X_train.columns if np.issubdtype(X_train[feature], np.number)]\n",
    "categorical_baseline = [feature for feature in X_train.columns if feature not in numeric_baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('numerical',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='median',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler(copy=True,\n",
       "                                                                                  wit...\n",
       "                                                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                                                 handle_unknown='ignore',\n",
       "                                                                                 n_values=None,\n",
       "                                                                                 sparse=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['Born', 'City', 'Pr/St',\n",
       "                                                   'Cntry', 'Nat', 'Hand',\n",
       "                                                   'Last Name', 'First Name',\n",
       "                                                   'Position', 'Team'])],\n",
       "                                   verbose=False)),\n",
       "                ('regressor',\n",
       "                 Ridge(alpha=100, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='auto', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_pipeline_ridge(regressor=None):\n",
    "    \"\"\"\n",
    "    Creates pipeline to perform transformations on numeric and categorical features \n",
    "    and pass into a Ridge Regression Model.\n",
    "    \"\"\"\n",
    "    numeric_features = numeric_baseline\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', impute.SimpleImputer(strategy='median')),\n",
    "        ('scaler', preprocessing.StandardScaler())])\n",
    "\n",
    "    categorical_features = categorical_baseline\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', impute.SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "        ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = compose.ColumnTransformer(transformers=[\n",
    "        ('numerical', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', regressor)])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "regressor = linear_model.Ridge(alpha=100, tol=0.001)\n",
    "pipeline = make_pipeline_ridge(regressor)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000746.27374221 1275507.29009635 1147796.88653346 1535379.04568226\n",
      " 1198276.48579808 1235176.51749743 1694561.60530901 1251440.38563199\n",
      " 1812870.53670413 1404180.70916489]\n",
      "\n",
      "1,455,593.57 average cross validation rmse\n"
     ]
    }
   ],
   "source": [
    "mean_squared_error_scorer = make_scorer(metrics.mean_squared_error)\n",
    "cross_val_bl = np.sqrt(cross_val_score(pipeline, \n",
    "                                       X_train, \n",
    "                                       y_train, \n",
    "                                       scoring=mean_squared_error_scorer,\n",
    "                                       cv=10))\n",
    "print(cross_val_bl)\n",
    "print('')\n",
    "print(f\"{add_commas(round(cross_val_bl.mean(), 2))} average cross validation rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,198,534.41 rmse on train dataset\n",
      "1,448,716.09 rmse on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_train)\n",
    "rmse_value_train = add_commas(round(np.sqrt(metrics.mean_squared_error(y_train, y_pred)), 2))\n",
    "print(f\"{rmse_value_train} rmse on train dataset\")\n",
    "\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "rmse_value_validation = add_commas(round(np.sqrt(metrics.mean_squared_error(y_validation, y_pred)), 2))\n",
    "print(f\"{rmse_value_validation} rmse on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$613,258.41 medae on train dataset\n",
      "$601,790.36 medae on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_train)\n",
    "medae_value_train = add_commas(round(metrics.median_absolute_error(y_train, y_pred), 2))\n",
    "print(f\"${medae_value_train} medae on train dataset\")\n",
    "\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "medae_value_validation = add_commas(round(metrics.median_absolute_error(y_validation, y_pred), 2))\n",
    "print(f\"${medae_value_validation} medae on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.21% mape on train dataset\n",
      "59.75% mape on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_train)\n",
    "mape_value_train = round(mape_metric(y_train, y_pred)*100, 2)\n",
    "print(f\"{mape_value_train}% mape on train dataset\")\n",
    "\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "mape_value_validation = round(mape_metric(y_validation, y_pred)*100, 2)\n",
    "print(f\"{mape_value_validation}% mape on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpredictive_features = ['ENG', 'Wide', 'Over', 'PSG', 'PSA', 'S.Dflct', 'G.Bkhd', 'Post', 'G.Dflct', 'CBar ', 'G.Slap', 'G.Snap', 'G.Wrst', 'G.Wrap', 'G.Tip', 'S.Bkhd', 'Min', 'S.Slap', 'Misc', 'Noise', 'DAP', 'Grit', 'PS', 'DPS', 'OPS', 'DSA', 'DSF', 'Game', 'Match', 'S.Snap', 'Maj', '1G', 'NPD', 'iPenDf', 'iPenD', 'iPenT', 'S.Wrst', 'S.Wrap', 'S.Tip', 'GWG', 'FOL.Down', 'OTG', 'PIM', 'iSF.1', 'iCF.1', 'Diff', 'Pct%', 'FOL.Close', 'TOI/GP.1', 'TOI/GP', 'TOI', 'Shifts', 'E+/-', 'sDist', '+/-', 'PTS', 'A2', 'A1', 'A', 'G', 'GP', 'Wt', 'Ht', 'iSF.2', 'Age', 'iFOW', 'iBLK', 'iFOL', 'dzFOL', 'nzFOW', 'nzFOL', 'ozFOW', 'ozFOL', 'dzFOW', 'FOL.Up', 'FOW.Up', 'iTKA', 'iGVA', 'iMiss', 'FOW.Down', 'iHF', 'FOW.Close', 'FO%', 'Position', 'Team', 'PEND', 'TOIX', 'GA', 'xGA', 'iSCF', 'iPEND', 'sDist.1', 'iHF.1', 'PDO', 'Hand', 'SCA', 'iTKA.1', 'SA', 'IPP%', 'ixG', 'FA', 'Pace', 'iGVA.1', 'SV%', 'RBF', 'PENT', 'F/60', 'GVA', 'TKA', 'FOW', 'Diff/60']\n",
    "\n",
    "# For KNN, we only want to include the numeric variables.\n",
    "# At the moment we do not want to consider some of the categorical \n",
    "# variables since KNN will not handle these naturally.\n",
    "numeric_knn = [feature for feature in X_train.columns if np.issubdtype(X_train[feature], np.number) \n",
    "                      and feature not in nonpredictive_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to fit the knn model, we will need to make sure that our pipeline is suited to handle the data for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline_knn(knn=None):\n",
    "    \"\"\"\n",
    "    Creates pipeline that performs separate transformations on the categorical and numerical features\n",
    "    for a KNN algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    # All predictive numeric features\n",
    "    numeric_features = numeric_knn\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        # Impute any missing values with the median.\n",
    "        ('imputer', impute.SimpleImputer(strategy='median')),\n",
    "        # Make sure that all values are Normalized for KNN. \n",
    "        # Since we are going to look at points that are similar in\n",
    "        # terms of euclidean space, we need all features on the same\n",
    "        # scale.\n",
    "        ('normalizer', preprocessing.Normalizer())])\n",
    "\n",
    "    preprocessor = compose.ColumnTransformer(transformers=[\n",
    "        ('numerical', numeric_transformer, numeric_features)])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('knn', knn)])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "pipeline_knn = make_pipeline_knn(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('preprocessor',\n",
       "                                              ColumnTransformer(n_jobs=None,\n",
       "                                                                remainder='drop',\n",
       "                                                                sparse_threshold=0.3,\n",
       "                                                                transformer_weights=None,\n",
       "                                                                transformers=[('numerical',\n",
       "                                                                               Pipeline(memory=None,\n",
       "                                                                                        steps=[('imputer',\n",
       "                                                                                                SimpleImputer(add_indicator=False,\n",
       "                                                                                                              copy=True,\n",
       "                                                                                                              fill_value=None,\n",
       "                                                                                                              missing_values=nan,\n",
       "                                                                                                              st...\n",
       "                                                                  metric_params=None,\n",
       "                                                                  n_jobs=None,\n",
       "                                                                  n_neighbors=5,\n",
       "                                                                  p=2,\n",
       "                                                                  weights='uniform'))],\n",
       "                                      verbose=False),\n",
       "                   iid='warn', n_iter=15, n_jobs=None,\n",
       "                   param_distributions={'knn__algorithm': ['ball_tree',\n",
       "                                                           'kd_tree', 'auto'],\n",
       "                                        'knn__n_neighbors': [8, 9, 10, 15],\n",
       "                                        'knn__weights': ['distance',\n",
       "                                                         'uniform']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_random_cv_knn():\n",
    "    \"\"\"\n",
    "    Define hyperparameter search space for KNN algorithm\n",
    "    Instantiate RandomizedSearchCV with the pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    algo = ['ball_tree', 'kd_tree', 'auto']\n",
    "    weights = ['distance', 'uniform']\n",
    "    neighbors = [8, 9, 10, 15]\n",
    "    hyperparameters = dict(knn__algorithm=algo,\n",
    "                          knn__n_neighbors=neighbors,\n",
    "                          knn__weights=weights)\n",
    "    \n",
    "    reg_random_cv = RandomizedSearchCV(pipeline_knn, \n",
    "                                       hyperparameters, \n",
    "                                       cv=5, \n",
    "                                       n_iter=15, \n",
    "                                       verbose=1,\n",
    "                                       random_state=42)\n",
    "    \n",
    "    return reg_random_cv\n",
    "\n",
    "model_knn = make_random_cv_knn()\n",
    "model_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2041209.3279363  1537846.54375191 1293287.92859458 1680304.59747979\n",
      " 1458831.35357341 1146695.47861003 1753619.98069111 1413846.89477454\n",
      " 1989899.973094   1341637.52508095]\n",
      "\n",
      "1,565,717.96 average cross validation rmse\n"
     ]
    }
   ],
   "source": [
    "mean_squared_error_scorer = make_scorer(metrics.mean_squared_error)\n",
    "cross_val_knn = np.sqrt(cross_val_score(model_knn.best_estimator_, \n",
    "                                       X_train, \n",
    "                                       y_train, \n",
    "                                       scoring=mean_squared_error_scorer,\n",
    "                                       cv=10))\n",
    "print(cross_val_knn)\n",
    "print('')\n",
    "print(f\"{add_commas(round(cross_val_knn.mean(), 2))} average cross validation rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,504,647.52 rmse on train dataset\n",
      "1,662,779.91 rmse on validation dataset\n"
     ]
    }
   ],
   "source": [
    "knn_y_pred = model_knn.best_estimator_.predict(X_train)\n",
    "rmse_value_train = add_commas(round(np.sqrt(metrics.mean_squared_error(y_train, knn_y_pred)), 2))\n",
    "print(f\"{rmse_value_train} rmse on train dataset\")\n",
    "\n",
    "knn_y_pred = model_knn.best_estimator_.predict(X_validation)\n",
    "rmse_value_validation = add_commas(round(np.sqrt(metrics.mean_squared_error(y_validation, knn_y_pred)), 2))\n",
    "print(f\"{rmse_value_validation} rmse on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$526,833.33 medae on train dataset\n",
      "$569,916.67 medae on validation dataset\n"
     ]
    }
   ],
   "source": [
    "knn_y_pred = model_knn.best_estimator_.predict(X_train)\n",
    "medae_value_train = add_commas(round(metrics.median_absolute_error(y_train, knn_y_pred), 2))\n",
    "print(f\"${medae_value_train} medae on train dataset\")\n",
    "\n",
    "knn_y_pred = model_knn.best_estimator_.predict(X_validation)\n",
    "medae_value_validation = add_commas(round(metrics.median_absolute_error(y_validation, knn_y_pred), 2))\n",
    "print(f\"${medae_value_validation:} medae on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.17% mape on train dataset\n",
      "64.29% mape on validation dataset\n"
     ]
    }
   ],
   "source": [
    "knn_y_pred = model_knn.best_estimator_.predict(X_train)\n",
    "mape_value_train = round(mape_metric(y_train, knn_y_pred)*100, 2)\n",
    "print(f\"{mape_value_train}% mape on train dataset\")\n",
    "\n",
    "knn_y_pred = model_knn.best_estimator_.predict(X_validation)\n",
    "mape_value_validation = round(mape_metric(y_validation, knn_y_pred)*100, 2)\n",
    "print(f\"{mape_value_validation}% mape on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are features which should have no affect on Salary, like First Name\n",
    "illogical_features = ['Born', 'City', 'Last Name', 'First Name', 'Cntry']\n",
    "# These are features which are repeated later on with updated stats\n",
    "redundant_features = ['TOI/GP', 'iCF', 'iSF', 'iSF.1', 'sDist', 'iHF', 'iGVA', 'iTKA', 'iBLK', 'iFOW', 'iFOL']\n",
    "# These are features found to be non-predictive using Parrt's rfpimp package\n",
    "\n",
    "drop_list = illogical_features + redundant_features\n",
    "\n",
    "numeric_rf = [feature for feature in X_train.columns if np.issubdtype(X_train[feature], np.number) \n",
    "                      and feature not in drop_list]\n",
    "categorical_rf = [feature for feature in X_train.columns if feature not in numeric_rf\n",
    "                       and feature not in drop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline_rf(regressor=None):\n",
    "    \"\"\"\n",
    "    Creates pipeline that performs separate transformations on the categorical and numerical features.\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_features = numeric_rf\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', impute.SimpleImputer(strategy='median'))])\n",
    "\n",
    "    categorical_features = categorical_rf\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', impute.SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "        ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = compose.ColumnTransformer(transformers=[\n",
    "        ('numerical', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', regressor)])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "regressor_rf = RandomForestRegressor()\n",
    "pipeline_rf = make_pipeline_rf(regressor_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('preprocessor',\n",
       "                                              ColumnTransformer(n_jobs=None,\n",
       "                                                                remainder='drop',\n",
       "                                                                sparse_threshold=0.3,\n",
       "                                                                transformer_weights=None,\n",
       "                                                                transformers=[('numerical',\n",
       "                                                                               Pipeline(memory=None,\n",
       "                                                                                        steps=[('imputer',\n",
       "                                                                                                SimpleImputer(add_indicator=False,\n",
       "                                                                                                              copy=True,\n",
       "                                                                                                              fill_value=None,\n",
       "                                                                                                              missing_values=nan,\n",
       "                                                                                                              st...\n",
       "                   iid='warn', n_iter=15, n_jobs=None,\n",
       "                   param_distributions={'regressor__bootstrap': ['True',\n",
       "                                                                 'False'],\n",
       "                                        'regressor__max_features': ['auto',\n",
       "                                                                    'sqrt'],\n",
       "                                        'regressor__min_samples_leaf': [2, 3, 5,\n",
       "                                                                        6, 7, 8,\n",
       "                                                                        9, 10],\n",
       "                                        'regressor__n_estimators': [100, 150,\n",
       "                                                                    200],\n",
       "                                        'regressor__oob_score': ['True',\n",
       "                                                                 'False']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_random_cv():\n",
    "    \"\"\"\n",
    "    Define hyperparameter search space\n",
    "    Instantiate RandomizedSearchCV with the pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    bootstrap = ['True', 'False']\n",
    "    oob_score = ['True', 'False']\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    min_samples_leaf = [2, 3, 5, 6 , 7, 8, 9, 10]\n",
    "    n_estimators = [100, 150, 200]\n",
    "    hyperparameters = dict(regressor__min_samples_leaf=min_samples_leaf,\n",
    "                          regressor__bootstrap=bootstrap,\n",
    "                          regressor__max_features=max_features,\n",
    "                          regressor__n_estimators=n_estimators,\n",
    "                          regressor__oob_score=oob_score)\n",
    "    reg_random_cv = RandomizedSearchCV(pipeline_rf, \n",
    "                                       hyperparameters, \n",
    "                                       cv=5, \n",
    "                                       n_iter=15, \n",
    "                                       verbose=1,\n",
    "                                       random_state=42)\n",
    "    \n",
    "    return reg_random_cv\n",
    "\n",
    "model_rf = make_random_cv()\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Scores for Best Estimator from Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1699842.03652417 1566877.11010208  889019.47973772 1469231.66789951\n",
      " 1310496.76462525 1276571.99125389 1464675.19690775 1125320.98528151\n",
      " 1670488.0922695  1527667.52512048]\n",
      "\n",
      "1,400,019.08 average cross validation rmse\n"
     ]
    }
   ],
   "source": [
    "mean_squared_error_scorer = make_scorer(metrics.mean_squared_error)\n",
    "cross_val_rf = np.sqrt(cross_val_score(model_rf.best_estimator_, \n",
    "                                       X_train, \n",
    "                                       y_train, \n",
    "                                       scoring=mean_squared_error_scorer,\n",
    "                                       cv=10))\n",
    "print(cross_val_rf)\n",
    "print('')\n",
    "print(f\"{add_commas(round(cross_val_rf.mean(), 2))} average cross validation rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679,719.07 rmse on train dataset\n",
      "1,343,844.35 rmse on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_train)\n",
    "rmse_value_train = add_commas(round(np.sqrt(metrics.mean_squared_error(y_train, y_pred)), 2))\n",
    "print(f\"{rmse_value_train} rmse on train dataset\")\n",
    "\n",
    "y_pred = model_rf.predict(X_validation)\n",
    "rmse_value_validation = add_commas(round(np.sqrt(metrics.mean_squared_error(y_validation, y_pred)), 2))\n",
    "print(f\"{rmse_value_validation} rmse on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$246,471.06 medae on train dataset\n",
      "$498,802.24 medae on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_train)\n",
    "medae_value_train = add_commas(round(metrics.median_absolute_error(y_train, y_pred), 2))\n",
    "print(f\"${medae_value_train} medae on train dataset\")\n",
    "\n",
    "y_pred = model_rf.predict(X_validation)\n",
    "medae_value_validation = add_commas(round(metrics.median_absolute_error(y_validation, y_pred), 2))\n",
    "print(f\"${medae_value_validation} medae on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.65% mape on train dataset\n",
      "48.96% mape on validation dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_train)\n",
    "mape_value_train = round(mape_metric(y_train, y_pred)*100, 2)\n",
    "print(f\"{mape_value_train}% mape on train dataset\")\n",
    "\n",
    "y_pred = model_rf.predict(X_validation)\n",
    "mape_value_validation = round(mape_metric(y_validation, y_pred)*100, 2)\n",
    "print(f\"{mape_value_validation}% mape on validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RMSE as our North Star evaluation metric to choose a model, we ended up choosing to continue with the Random Forest Model because it consistently had the best validation set test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Features\n"
     ]
    }
   ],
   "source": [
    "from rfpimp import *\n",
    "\n",
    "# Using the rfpimp package by ParrT, we look at our best model and estimate feature importance\n",
    "I = importances(model_rf.best_estimator_, X_train, y_train)\n",
    "I.reset_index(inplace = True)\n",
    "\n",
    "# These features deemed 'not important are dropped'\n",
    "# It doesn't affect the training score much, but a simpler model generalizes better\n",
    "print(len(list(I.loc[(I.Importance <= 0)]['Feature'])), 'Features')\n",
    "new_drop_list = list(I.loc[(I.Importance <= 0)]['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('preprocessor',\n",
       "                                              ColumnTransformer(n_jobs=None,\n",
       "                                                                remainder='drop',\n",
       "                                                                sparse_threshold=0.3,\n",
       "                                                                transformer_weights=None,\n",
       "                                                                transformers=[('numerical',\n",
       "                                                                               Pipeline(memory=None,\n",
       "                                                                                        steps=[('imputer',\n",
       "                                                                                                SimpleImputer(add_indicator=False,\n",
       "                                                                                                              copy=True,\n",
       "                                                                                                              fill_value=None,\n",
       "                                                                                                              missing_values=nan,\n",
       "                                                                                                              st...\n",
       "                   iid='warn', n_iter=15, n_jobs=None,\n",
       "                   param_distributions={'regressor__bootstrap': ['True',\n",
       "                                                                 'False'],\n",
       "                                        'regressor__max_features': ['auto',\n",
       "                                                                    'sqrt'],\n",
       "                                        'regressor__min_samples_leaf': [2, 3, 5,\n",
       "                                                                        6, 7, 8,\n",
       "                                                                        9, 10],\n",
       "                                        'regressor__n_estimators': [100, 150,\n",
       "                                                                    200],\n",
       "                                        'regressor__oob_score': ['True',\n",
       "                                                                 'False']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain the model on the new drop list\n",
    "numeric_rf = [feature for feature in X_train.columns if np.issubdtype(X_train[feature], np.number) \n",
    "                      and feature not in new_drop_list]\n",
    "categorical_rf = [feature for feature in X_train.columns if feature not in numeric_rf\n",
    "                       and feature not in new_drop_list]\n",
    "\n",
    "regressor_rf = RandomForestRegressor()\n",
    "pipeline_rf = make_pipeline_rf(regressor_rf)\n",
    "model_rf = make_random_cv()\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Deliver\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Median Absolute Error Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$491,953.97 medae on test dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_test)\n",
    "medae_value_test = add_commas(round(metrics.median_absolute_error(y_test, y_pred), 2))\n",
    "print(f\"${medae_value_test:} medae on test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Mean Squared Error Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,498,301.19 rmse on test dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_test)\n",
    "rmse_value_test = add_commas(round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)), 2))\n",
    "print(f\"{rmse_value_test} rmse on test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Percentage Error Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.83% mape on test dataset\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict(X_test)\n",
    "mape_value_test = round(mape_metric(y_test, y_pred)*100, 2)\n",
    "print(f\"{mape_value_test}% mape on test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='font-size:18px'> \n",
    "To predict NHL player salaries, we fitted three regressor models, a Lasso Regression model, a KNN model, and a Random Forest model. After running our models against each other, we looked at three different evaluation metrics for each. Each evaluation metric told us something uniquely useful about our models. Median Absolute Error (MedAE) is the most useful for external communication of our model. To be able to say that we were off by \\\\$100,000 is something that we could tell a hockey General Manager to help them understand what is going on. Median Absolute Percent Error (MAPE) is used for internal understaning of our model. It is a metric to understand error like MedAE, but it is easier for us as data scientists to say our model is off by 53\\% rather than off by \\\\$557,132. Finally, our 'North Star' metric that we use to pick a model Root Mean Square Error (RMSE). RMSE minimizes the loss, and unlike MAPE, RMSE is symmetric. MAPE sufferes from asymmetry, which means that predictions which undershoot the true value are not valued the same as predictions which overshoot the true value by the same amount.</span>\n",
    "\n",
    "<span style='font-size:18px'> \n",
    "Our single best model was our Random Forest model which had the lowest RMSE of 1.5 million. This establishes that we chose this model, to interpret it, we turn to the MAPE of 57\\% and \\\\$648,000 on the test set. We can take this model to a NHL General Manager and give them a way to predict salary in the ever-changing salary cap environment of the NHL.</span>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
